import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.base import TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import string
import en_core_web_sm


def main():
    # File importation
    try:
        df = pd.read_excel('/Users/jerry/Downloads/ins_data.xlsx')
    except:
        print('Excel file not reading')

    # Create our list of punctuation marks
    punctuations = string.punctuation

    # Create our list of stopwords
    nlp = spacy.load('en_core_web_sm')
    stop_words = spacy.lang.en.stop_words.STOP_WORDS

    # Load English tokenizer, tagger, parser, NER and word vectors
    parser = English()
    
    # Creating our tokenizer function
    def spacy_tokenizer(sentence):
        # Creating our token object, which is used to create documents with linguistic annotations
        mytokens = parser(sentence)
         
        # Lemmatizing each token and converting each token into lowercase
        mytokens = [ word.lemma_.lower().strip() if word.lemma_ != "-PRON-" else word.lower_ for word in mytokens]

        #Removing stop words
        mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]

        return mytokens
    
    # Custom transformer using spaCy
    class predictors(TransformerMixin):
        def transform(self, X, **transform_params):
            # Cleaning text
            return [clean_text(text) for text in X]
        
        def fit(self, X, y=None, **fit_params):
            return self
        
        def get_params(self, deep=True):
            return {}
        
    # Basic function to clean the text    
    def clean_text(text):
        # Removing spaces and converting text into lowercase
        return str(text).strip().lower()
    
    bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))

    fdidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)
    
    X = df['message'] # the features we want to analyze
    ylabels = df['feedback'] # the lables, or answers, we want to test against
    
    X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.8)
    
    # Logistical Regression Classifier
    classifier = LogisticRegression()
    
    # Create pipeline using Bag of Words
    pipe = Pipeline([('cleaner', predictors()),
                     ('vectorizer', bow_vector),
                     ('classifier', classifier)])
    
    # model generation
    pipe.fit(X_train, y_train)
    
    #Predicting with a test dataset
    predicted = pipe.predict(X_test)
    
    # Accuracy refers to the percentage of the total predictions our model makes that are completely correct.
    print("Logistic Regression Accuracy:",metrics.accuracy_score(y_test, predicted))
    # Precision describes the ratio of true positives to true positives plus false positives in our predictions.
    print("Logistic Regression Precision:",metrics.precision_score(y_test, predicted))
    # Recall describes the ratio of true positives to true positives plus false negatives in our predictions.
    print("Logistic Regression Recall:",metrics.recall_score(y_test, predicted))

if __name__ == '__main__':
    main()
